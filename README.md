ðŸš¨ Credit Card Fraud Detection
Binary Classification | Imbalanced Learning | Business-Driven ML
<p align="center"> <img src="images/image.webp" alt="Fraud Detection Banner" width="50%"> </p>

Fraud detection is not a modeling problem â€” it is a decision-making problem under extreme imbalance.

This project demonstrates an end-to-end, production-oriented machine learning pipeline for detecting fraudulent credit card transactions, focusing on engineering choices, evaluation correctness, and business risk optimization.

ðŸ”¥ Why This Project Matters

Fraud transactions are < 0.2% of all data

Accuracy is misleading

Missing fraud is financially expensive

Real systems care about recall, cost, and explainability, not leaderboard scores

This repository shows how fraud detection is actually done in the real world.

ðŸ§  Problem Definition

We model fraud detection as a binary classification problem:
| Label | Meaning                |
| ----- | ---------------------- |
| `0`   | Legitimate Transaction |
| `1`   | Fraudulent Transaction |

ðŸ“Š Dataset Overview (Numerical Transactions)

Dataset: Credit Card Fraud Detection

Source: Kaggle

File: creditcard.csv

Total Transactions: ~284,807

Fraud Cases: ~492

ðŸ” Features

V1 â†’ V28: PCA-transformed anonymized features

Amount: Transaction value

Class: Target label

âš ï¸ Severe class imbalance (<0.2%) makes this dataset realistic and dangerous.


ðŸ§© Core Challenges Addressed

âœ” Extreme class imbalance

âœ” Hardware & performance constraints

âœ” Metric selection beyond accuracy

âœ” Probability calibration

âœ” Business-aware thresholding

âœ” Model explainability (SHAP)


ðŸ¤– Models Implemented

1ï¸âƒ£ Logistic Regression (Baseline)

Trained on original imbalanced data

Demonstrates why accuracy fails

ðŸ“‰ Result:

âœ” High accuracy

âŒ Almost zero fraud recall

2ï¸âƒ£ Logistic Regression + SMOTE

Uses Synthetic Minority Over-sampling Technique

Balances learning without touching test data

ðŸ“ˆ Result:

âœ” Significant recall improvement

âœ” Better minority-class learning

3ï¸âƒ£ ðŸš€ XGBoost (Industry-Grade Model)

Implemented in xg_boost.py

âœ” Captures non-linear fraud patterns

âœ” Uses scale_pos_weight for imbalance

âœ” Optimized using PR-AUC, not accuracy

ðŸ“Š Outcome:

Best precisionâ€“recall tradeoff

Production-ready behavior

ðŸ“ˆ Evaluation Metrics (Chosen on Purpose)

Accuracy is not used for decision-making.

| Metric    | Why It Matters                 |
| --------- | ------------------------------ |
| Precision | Avoid blocking legit users     |
| Recall    | Catch fraud (highest priority) |
| F1-Score  | Balance precision & recall     |
| ROC-AUC   | Ranking quality                |
| PR-AUC    | True performance on imbalance  |

ðŸ§  Probability Calibration & Business Thresholding

Instead of using the default 0.5 threshold:

âœ” Model probabilities are calibrated
âœ” Precisionâ€“Recall curve is analyzed
âœ” F1-optimal & cost-aware threshold is selected

ðŸ§® Cost Matrix Logic

âŒ False Negative (Missed Fraud) â†’ High Cost

âš  False Positive (Blocked Legit) â†’ Lower Cost

This aligns predictions with financial reality, not math purity.

ðŸ“Š Visual Evidence (Included)

| Visualization                   | Purpose                |
| ------------------------------- | ---------------------- |
| Confusion Matrix (Before SMOTE) | Shows imbalance damage |
| Confusion Matrix (After SMOTE)  | Recall improvement     |
| ROC Curve                       | Ranking capability     |
| Precisionâ€“Recall Curve          | Minority performance   |
| Business Threshold Matrix       | Cost-aware decisions   |
| XGBoost PR Curve                | Advanced model gains   |
| SHAP Plots                      | Model transparency     |


## ðŸ–¼ï¸ Visual Results & Artifacts

| Image Preview | File Name | Description |
|--------------|----------|-------------|
| ![](images/before_smote.png) | `before_smote.png` | Confusion matrix before SMOTE showing severe class imbalance |
| ![](images/after_smote.png) | `after_smote.png` | Confusion matrix after SMOTE with improved fraud recall |
| ![](images/roc_curve.png) | `roc_curve.png` | ROC curve showing class separation capability |
| ![](images/pr_curve.png) | `pr_curve.png` | Precisionâ€“Recall curve highlighting minority-class performance |
| ![](images/bussiness_threesold.png) | `bussiness_threesold.png` | Confusion matrix using business-optimized decision threshold |
| ![](images/xg_boost.png) | `xg_boost.png` | Baseline XGBoost model performance |
| ![](images/pr_xgboost.png) | `pr_xgboost.png` | Precisionâ€“Recall curve for XGBoost model |
| ![](images/xgboost_optimize.png) | `xgboost_optimize.png` | Optimized XGBoost results after threshold tuning |
| ![](images/shap.png) | `shap.png` | SHAP feature importance (global explainability) |
| ![](images/shap_Output.png) | `shap_Output.png` | SHAP output explaining individual predictions |
| ![](images/bert_nlp.png) | `bert_nlp.png` | BERT-based NLP transaction classification preview |



ðŸ” Explainability with SHAP

âœ” Feature-level contribution analysis

âœ” Transaction-level decision explanation

âœ” Required for banking & regulatory trust

This makes the model auditable, not a black box.

ðŸ§  NLP-Based Fraud Detection (Text Transactions)

In addition to numeric data, the project includes an experimental NLP pipeline using BERT embeddings.

ðŸ”¹ Dataset

File: data.csv

Samples: 20 (balanced)

Purpose: Learning & demonstration

ðŸ”¹ NLP Flow

Transaction Text

â†’ BERT Tokenization

â†’ CLS Embedding

â†’ Binary Classifier



âš ï¸ This module demonstrates architecture, not real-world scale.

ðŸ› ï¸ Tech Stack

Python

NumPy, Pandas

Scikit-learn

Imbalanced-learn (SMOTE)

XGBoost

Matplotlib

SHAP

Transformers (BERT)

ðŸš€ Key Engineering Takeaways

âœ” Accuracy is misleading

âœ” Recall beats precision in fraud

âœ” SMOTE changes learning dynamics

âœ” Thresholds matter more than models

âœ” XGBoost dominates linear models

âœ” Explainability is non-negotiable

ðŸ”® Upcoming Updates (Work in Progress)

Ensemble learning (Logistic + XGBoost)

Hyperparameter optimization

Time-aware validation

Real-time prediction API (FastAPI)

Streamlit dashboard

Production-ready deployment flow

This commit is an update â€” more advanced versions are coming.

âœ… Final Note

This project is not a toy ML notebook.

It reflects real-world fraud detection thinking â€” the kind expected in:

Technical interviews

Industry ML roles

Academic evaluation

Serious GitHub portfolios

â­ If you understand why each decision was made here, you already think beyond beginner ML.
