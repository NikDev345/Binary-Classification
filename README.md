ğŸš¨ Credit Card Fraud Detection
Binary Classification | Imbalanced Learning | Business-Driven ML
<p align="center"> <img src="images/image.webp" alt="Fraud Detection Banner" width="50%"> </p>

Fraud detection is not a modeling problem â€” it is a decision-making problem under extreme imbalance.
This project demonstrates an end-to-end, production-oriented machine learning pipeline for detecting fraudulent credit card transactions, focusing on engineering choices, evaluation correctness, and business risk optimization.

ğŸ”¥ Why This Project Matters

Fraud transactions are < 0.2% of all data

Accuracy is misleading

Missing fraud is financially expensive

Real systems care about recall, cost, and explainability, not leaderboard scores

This repository shows how fraud detection is actually done in the real world.

ğŸ§  Problem Definition

We model fraud detection as a binary classification problem:
| Label | Meaning                |
| ----- | ---------------------- |
| `0`   | Legitimate Transaction |
| `1`   | Fraudulent Transaction |

ğŸ“Š Dataset Overview (Numerical Transactions)

Dataset: Credit Card Fraud Detection

Source: Kaggle

File: creditcard.csv

Total Transactions: ~284,807

Fraud Cases: ~492

ğŸ” Features

V1 â†’ V28: PCA-transformed anonymized features

Amount: Transaction value

Class: Target label

âš ï¸ Severe class imbalance (<0.2%) makes this dataset realistic and dangerous.


ğŸ§© Core Challenges Addressed

âœ” Extreme class imbalance
âœ” Hardware & performance constraints
âœ” Metric selection beyond accuracy
âœ” Probability calibration
âœ” Business-aware thresholding
âœ” Model explainability (SHAP)


flowchart LR
A[Raw Data] --> B[EDA & Sampling]
B --> C[Train-Test Split]
C --> D[Baseline Model]
D --> E[SMOTE Balancing]
E --> F[Improved Model]
F --> G[Threshold Optimization]
G --> H[XGBoost Advanced Model]
H --> I[Explainability & Risk Decisions]


ğŸ¤– Models Implemented
1ï¸âƒ£ Logistic Regression (Baseline)

Trained on original imbalanced data

Demonstrates why accuracy fails

ğŸ“‰ Result:
âœ” High accuracy
âŒ Almost zero fraud recall

2ï¸âƒ£ Logistic Regression + SMOTE

Uses Synthetic Minority Over-sampling Technique

Balances learning without touching test data

ğŸ“ˆ Result:
âœ” Significant recall improvement
âœ” Better minority-class learning

3ï¸âƒ£ ğŸš€ XGBoost (Industry-Grade Model)

Implemented in xg_boost.py

âœ” Captures non-linear fraud patterns
âœ” Uses scale_pos_weight for imbalance
âœ” Optimized using PR-AUC, not accuracy

ğŸ“Š Outcome:

Best precisionâ€“recall tradeoff

Production-ready behavior

ğŸ“ˆ Evaluation Metrics (Chosen on Purpose)

Accuracy is not used for decision-making.

| Metric    | Why It Matters                 |
| --------- | ------------------------------ |
| Precision | Avoid blocking legit users     |
| Recall    | Catch fraud (highest priority) |
| F1-Score  | Balance precision & recall     |
| ROC-AUC   | Ranking quality                |
| PR-AUC    | True performance on imbalance  |

ğŸ§  Probability Calibration & Business Thresholding

Instead of using the default 0.5 threshold:

âœ” Model probabilities are calibrated
âœ” Precisionâ€“Recall curve is analyzed
âœ” F1-optimal & cost-aware threshold is selected

ğŸ§® Cost Matrix Logic

âŒ False Negative (Missed Fraud) â†’ High Cost

âš  False Positive (Blocked Legit) â†’ Lower Cost

This aligns predictions with financial reality, not math purity.

ğŸ“Š Visual Evidence (Included)

| Visualization                   | Purpose                |
| ------------------------------- | ---------------------- |
| Confusion Matrix (Before SMOTE) | Shows imbalance damage |
| Confusion Matrix (After SMOTE)  | Recall improvement     |
| ROC Curve                       | Ranking capability     |
| Precisionâ€“Recall Curve          | Minority performance   |
| Business Threshold Matrix       | Cost-aware decisions   |
| XGBoost PR Curve                | Advanced model gains   |
| SHAP Plots                      | Model transparency     |


## ğŸ–¼ï¸ Visual Results & Artifacts

| Image Preview | File Name | Description |
|--------------|----------|-------------|
| ![](images/before_smote.png) | `before_smote.png` | Confusion matrix before SMOTE showing severe class imbalance |
| ![](images/after_smote.png) | `after_smote.png` | Confusion matrix after SMOTE with improved fraud recall |
| ![](images/roc_curve.png) | `roc_curve.png` | ROC curve showing class separation capability |
| ![](images/pr_curve.png) | `pr_curve.png` | Precisionâ€“Recall curve highlighting minority-class performance |
| ![](images/bussiness_threesold.png) | `bussiness_threesold.png` | Confusion matrix using business-optimized decision threshold |
| ![](images/xg_boost.png) | `xg_boost.png` | Baseline XGBoost model performance |
| ![](images/pr_xgboost.png) | `pr_xgboost.png` | Precisionâ€“Recall curve for XGBoost model |
| ![](images/xgboost_optimize.png) | `xgboost_optimize.png` | Optimized XGBoost results after threshold tuning |
| ![](images/shap.png) | `shap.png` | SHAP feature importance (global explainability) |
| ![](images/shap_Output.png) | `shap_Output.png` | SHAP output explaining individual predictions |
| ![](images/bert_nlp.png) | `bert_nlp.png` | BERT-based NLP transaction classification preview |



ğŸ” Explainability with SHAP

âœ” Feature-level contribution analysis
âœ” Transaction-level decision explanation
âœ” Required for banking & regulatory trust

This makes the model auditable, not a black box.

ğŸ§  NLP-Based Fraud Detection (Text Transactions)

In addition to numeric data, the project includes an experimental NLP pipeline using BERT embeddings.

ğŸ”¹ Dataset

File: data.csv

Samples: 20 (balanced)

Purpose: Learning & demonstration

ğŸ”¹ NLP Flow
Transaction Text
â†’ BERT Tokenization
â†’ CLS Embedding
â†’ Binary Classifier


âš ï¸ This module demonstrates architecture, not real-world scale.

ğŸ› ï¸ Tech Stack

Python

NumPy, Pandas

Scikit-learn

Imbalanced-learn (SMOTE)

XGBoost

Matplotlib

SHAP

Transformers (BERT)

ğŸš€ Key Engineering Takeaways

âœ” Accuracy is misleading
âœ” Recall beats precision in fraud
âœ” SMOTE changes learning dynamics
âœ” Thresholds matter more than models
âœ” XGBoost dominates linear models
âœ” Explainability is non-negotiable

ğŸ”® Upcoming Updates (Work in Progress)

Ensemble learning (Logistic + XGBoost)

Hyperparameter optimization

Time-aware validation

Real-time prediction API (FastAPI)

Streamlit dashboard

Production-ready deployment flow

This commit is an update â€” more advanced versions are coming.

âœ… Final Note

This project is not a toy ML notebook.
It reflects real-world fraud detection thinking â€” the kind expected in:

Technical interviews

Industry ML roles

Academic evaluation

Serious GitHub portfolios

â­ If you understand why each decision was made here, you already think beyond beginner ML.
