ðŸš¨ Credit Card Fraud Detection
Binary Classification using Machine Learning
ðŸ“Œ Project Overview

Credit card fraud detection is a critical real-world problem where fraudulent transactions are extremely rare compared to legitimate ones.
This project builds an end-to-end binary classification system to identify fraudulent credit card transactions using machine learning, while carefully handling:

Severe class imbalance

Performance constraints

Business-driven evaluation metrics

This is not just model training â€” it demonstrates engineering-level ML thinking.

ðŸŽ¯ Problem Statement

Given a dataset of credit card transactions, classify each transaction as:

0 â†’ Legitimate Transaction

1 â†’ Fraudulent Transaction

This is a binary classification problem with highly imbalanced data, making it realistic and challenging.

ðŸ“Š Dataset Information

Dataset Name: Credit Card Fraud Detection

Source: Kaggle

File Name: creditcard.csv

Total Transactions: ~284,000

Fraudulent Transactions: ~492

ðŸ“¥ Dataset Download Instructions

Go to Kaggle

Search for â€œCredit Card Fraud Detectionâ€

Download the dataset

Place the file in the project root as:

creditcard.csv

ðŸ§¾ Dataset Description

Each row represents one credit card transaction

Features V1 to V28 are anonymized (PCA-transformed)

Amount represents the transaction value

Class is the target variable:

0 â†’ Legitimate

1 â†’ Fraud

âš ï¸ Key Challenge: Class Imbalance

Fraud transactions represent less than 0.2% of the data

Accuracy alone is misleading

Special techniques are required to detect fraud effectively

This project explicitly addresses this issue.

ðŸ§  Machine Learning Approach
âœ” Learning Type

Supervised Learning

âœ” Problem Type

Binary Classification

âœ” Models Used

Logistic Regression (Baseline)

Logistic Regression with SMOTE

XGBoost (Advanced Model)



âš™ï¸ Project Workflow

Load and analyze the dataset

Apply sampling for hardware efficiency

Perform stratified trainâ€“test split

Train baseline model (Before SMOTE)

Apply SMOTE to balance the dataset

Train model after SMOTE

Evaluate using proper metrics

Calibrate probabilities

Apply business-driven decision threshold

Train and evaluate XGBoost advanced model

ðŸ“ˆ Evaluation Metrics

To properly evaluate an imbalanced dataset, the following metrics are used:

Precision

Recall

F1-Score

Confusion Matrix

ROCâ€“AUC

Precisionâ€“Recall AUC

ðŸ“Œ Recall is prioritized, as missing a fraudulent transaction is more costly than flagging a legitimate one.

ðŸ” Baseline Results (Before SMOTE)

The baseline Logistic Regression model is trained on the original imbalanced dataset.

ðŸ“· Confusion Matrix â€” Before SMOTE

ðŸ”¹ Observation

Very high accuracy

Very poor fraud recall

Model heavily biased toward legitimate transactions

ðŸ”¥ Improved Results (After SMOTE)

SMOTE (Synthetic Minority Over-sampling Technique) is applied before training.

ðŸ“· Confusion Matrix â€” After SMOTE

ðŸ”¹ Observation

Improved fraud detection

Better recall for fraudulent transactions

More balanced learning

ðŸ“Š ROC Curve & Precisionâ€“Recall Curve

The project includes:

ROC Curve â†’ Measures class separation

Precisionâ€“Recall Curve â†’ More informative for imbalanced data




ðŸ§  Probability Calibration & Business Threshold

Model probabilities are calibrated to reflect realistic fraud risk

A custom decision threshold is applied instead of the default 0.5

Improves fraud detection based on business risk considerations

ðŸš€ Advanced Model: XGBoost

To further improve fraud detection performance, an advanced gradient boosting model is added.

ðŸ“Œ Implementation Details

New file added:

xg_boost.py


Uses XGBoost for non-linear learning

Handles class imbalance using scale_pos_weight

Optimized using PR-AUC, not accuracy

ðŸ“· XGBoost Results
ðŸ”¹ XGBoost Performance Visualization

ðŸ”¹ Precisionâ€“Recall Curve (XGBoost)

ðŸ”¹ Observation

Strong improvement in fraud detection

Better precisionâ€“recall tradeoff

Industry-grade performance for imbalanced classification

### ðŸ“· Confusion Matrix â€” Before SMOTE

![Before SMOTE Confusion Matrix](images/before_smote.png)

ðŸ”¥ Improved Results (After SMOTE)
### ðŸ“· Confusion Matrix â€” After SMOTE

![After SMOTE Confusion Matrix](images/after_smote.png)

ðŸ“Š ROC Curve & Precisionâ€“Recall Curve
### ðŸ“ˆ ROC Curve
![ROC Curve](images/roc_curve.png)

### ðŸ“‰ Precisionâ€“Recall Curve
![PR Curve](images/pr_curve.png)

ðŸ§  Probability Calibration & Business Threshold
### ðŸ§  Business-Driven Threshold

![Business Threshold](images/bussiness_threesold.png)

ðŸš€ Advanced Model: XGBoost
## ðŸš€ Advanced Model: XGBoost

A new advanced model is implemented in `xg_boost.py` using XGBoost to capture non-linear fraud patterns.

ðŸ“· XGBoost Visualizations
### ðŸ“Š XGBoost Performance

![XGBoost Results](images/xg_boost.png)

### ðŸ“‰ Precisionâ€“Recall Curve (XGBoost)

![PR XGBoost](images/pr_xgboost.png)


| Risk Level | Action                 |
| ---------- | ---------------------- |
| LOW_RISK   | Allow transaction      |
| REVIEW     | Flag for manual review |
| HIGH_RISK  | Block transaction      |


ðŸ–¼ï¸ Image Assets Summary 
| Image                     | Description                                                                            |
| ------------------------- | -------------------------------------------------------------------------------------- |
| `before_smote.png`        | Confusion matrix before applying SMOTE, showing the impact of severe class imbalance   |
| `after_smote.png`         | Confusion matrix after applying SMOTE, demonstrating improved minority-class detection |
| `roc_curve.png`           | ROC curve showing the ranking ability of the baseline fraud detection model            |
| `pr_curve.png`            | Precisionâ€“Recall curve highlighting minority-class performance of the baseline model   |
| `bussiness_threesold.png` | Confusion matrix using a business-optimized probability threshold                      |
| `xg_boost.png`            | Baseline XGBoost fraud detection performance using the default threshold               |
| `pr_xgboost.png`          | Precisionâ€“Recall curve for the XGBoost fraud detection model                           |
| `xg_boost_optimize.png`   | Optimized XGBoost results after threshold tuning and cost-aware decisioning            |
| `shap.png`                | SHAP explainability plot showing feature contributions to fraud predictions            |



ðŸ§° Technologies & Libraries Used

Python

NumPy

Pandas

Scikit-learn

Imbalanced-learn (SMOTE)

XGBoost

Matplotlib

Seaborn

## ðŸ“Š Dataset Information

This project uses a **text-based transaction dataset** for binary fraud detection using **BERT NLP embeddings**.

### ðŸ”¹ Dataset File
- **File name:** `data.csv`
- **Total samples:** 20
- **Classes:**
  - `0` â†’ Legitimate transaction
  - `1` â†’ Fraudulent transaction

### ðŸ”¹ Class Distribution
- Legitimate transactions: 10
- Fraudulent transactions: 10

The dataset is intentionally kept small and balanced to:
- Avoid stratification errors
- Demonstrate BERT-based NLP classification clearly
- Focus on model pipeline understanding rather than data volume

> âš ï¸ Note: This dataset is for **learning and demonstration purposes only**.  
> Real-world fraud detection requires large, imbalanced datasets with cost-sensitive evaluation.

---

## ðŸ§  Text-Based Fraud Detection (NLP)

Transaction descriptions are processed using **BERT (bert-base-uncased)** as a feature extractor.  
The **[CLS] token embedding** is used to represent each transaction sentence, which is then passed to a traditional machine learning classifier.

### ðŸ”¹ NLP Flow
1. Raw transaction text
2. BERT tokenization
3. CLS embedding extraction
4. Binary classification (Fraud / Legit)

---

## ðŸ”€ Trainâ€“Test Split Strategy

To preserve class balance, **stratified sampling** is used:

python
train_test_split(
    X,
    y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

![Dataset Preview](images/bert_nlp.png)

ðŸš€ Key Learnings

Accuracy is unreliable for imbalanced datasets

Handling class imbalance is essential in fraud detection

SMOTE significantly improves minority class detection

Threshold tuning is more important than raw scores

Advanced models like XGBoost capture complex fraud patterns

Real-world ML requires balancing performance and constraints

ðŸ”® Future Improvements

Ensemble learning (Logistic + XGBoost)

Hyperparameter tuning

Time-aware validation

SHAP explainability

Real-time fraud detection API

Web deployment using Streamlit

âœ… Conclusion

This project demonstrates a complete, real-world fraud detection pipeline using binary classification.
It emphasizes correct evaluation, imbalance handling, and engineering decisions, making it suitable for:

Academic submission

GitHub portfolios

Technical interviews

and Personal projects 
