ğŸš¨ Credit Card Fraud Detection
Binary Classification using Machine Learning
ğŸ“Œ Project Overview

Credit card fraud detection is a critical real-world problem where fraudulent transactions are extremely rare compared to legitimate ones.
This project builds an end-to-end binary classification system to identify fraudulent credit card transactions using machine learning, while carefully handling:

Severe class imbalance

Performance constraints

Business-driven evaluation metrics

This is not just model training â€” it demonstrates engineering-level ML thinking.

ğŸ¯ Problem Statement

Given a dataset of credit card transactions, classify each transaction as:

0 â†’ Legitimate Transaction

1 â†’ Fraudulent Transaction

This is a binary classification problem with highly imbalanced data, making it realistic and challenging.

ğŸ“Š Dataset Information

Dataset Name: Credit Card Fraud Detection

Source: Kaggle

File Name: creditcard.csv

Total Transactions: ~284,000

Fraudulent Transactions: ~492

ğŸ“¥ Dataset Download Instructions

Go to Kaggle

Search for â€œCredit Card Fraud Detectionâ€

Download the dataset

Place the file in the project root as:

creditcard.csv

ğŸ§¾ Dataset Description

Each row represents one credit card transaction

Features V1 to V28 are anonymized (PCA-transformed)

Amount represents the transaction value

Class is the target variable:

0 â†’ Legitimate

1 â†’ Fraud

âš ï¸ Key Challenge: Class Imbalance

Fraud transactions represent less than 0.2% of the data

Accuracy alone is misleading

Special techniques are required to detect fraud effectively

This project explicitly addresses this issue.

ğŸ§  Machine Learning Approach
âœ” Learning Type

Supervised Learning

âœ” Problem Type

Binary Classification

âœ” Models Used

Logistic Regression (Baseline)

Logistic Regression with SMOTE

XGBoost (Advanced Model)

âš™ï¸ Project Workflow

Load and analyze the dataset

Apply sampling for hardware efficiency

Perform stratified trainâ€“test split

Train baseline model (Before SMOTE)

Apply SMOTE to balance the dataset

Train model after SMOTE

Evaluate using proper metrics

Calibrate probabilities

Apply business-driven decision threshold

Train and evaluate XGBoost advanced model

ğŸ“ˆ Evaluation Metrics

To properly evaluate an imbalanced dataset, the following metrics are used:

Precision

Recall

F1-Score

Confusion Matrix

ROCâ€“AUC

Precisionâ€“Recall AUC

ğŸ“Œ Recall is prioritized, as missing a fraudulent transaction is more costly than flagging a legitimate one.

ğŸ” Baseline Results (Before SMOTE)

The baseline Logistic Regression model is trained on the original imbalanced dataset.

ğŸ“· Confusion Matrix â€” Before SMOTE

ğŸ”¹ Observation

Very high accuracy

Very poor fraud recall

Model heavily biased toward legitimate transactions

ğŸ”¥ Improved Results (After SMOTE)

SMOTE (Synthetic Minority Over-sampling Technique) is applied before training.

ğŸ“· Confusion Matrix â€” After SMOTE

ğŸ”¹ Observation

Improved fraud detection

Better recall for fraudulent transactions

More balanced learning

ğŸ“Š ROC Curve & Precisionâ€“Recall Curve

The project includes:

ROC Curve â†’ Measures class separation

Precisionâ€“Recall Curve â†’ More informative for imbalanced data




ğŸ§  Probability Calibration & Business Threshold

Model probabilities are calibrated to reflect realistic fraud risk

A custom decision threshold is applied instead of the default 0.5

Improves fraud detection based on business risk considerations

ğŸš€ Advanced Model: XGBoost

To further improve fraud detection performance, an advanced gradient boosting model is added.

ğŸ“Œ Implementation Details

New file added:

xg_boost.py


Uses XGBoost for non-linear learning

Handles class imbalance using scale_pos_weight

Optimized using PR-AUC, not accuracy

ğŸ“· XGBoost Results
ğŸ”¹ XGBoost Performance Visualization

ğŸ”¹ Precisionâ€“Recall Curve (XGBoost)

ğŸ”¹ Observation

Strong improvement in fraud detection

Better precisionâ€“recall tradeoff

Industry-grade performance for imbalanced classification

### ğŸ“· Confusion Matrix â€” Before SMOTE

![Before SMOTE Confusion Matrix](images/before_smote.png)

ğŸ”¥ Improved Results (After SMOTE)
### ğŸ“· Confusion Matrix â€” After SMOTE

![After SMOTE Confusion Matrix](images/after_smote.png)

ğŸ“Š ROC Curve & Precisionâ€“Recall Curve
### ğŸ“ˆ ROC Curve
![ROC Curve](images/roc_curve.png)

### ğŸ“‰ Precisionâ€“Recall Curve
![PR Curve](images/pr_curve.png)

ğŸ§  Probability Calibration & Business Threshold
### ğŸ§  Business-Driven Threshold

![Business Threshold](images/bussiness_threesold.png)

ğŸš€ Advanced Model: XGBoost
## ğŸš€ Advanced Model: XGBoost

A new advanced model is implemented in `xg_boost.py` using XGBoost to capture non-linear fraud patterns.

ğŸ“· XGBoost Visualizations
### ğŸ“Š XGBoost Performance

![XGBoost Results](images/xg_boost.png)

### ğŸ“‰ Precisionâ€“Recall Curve (XGBoost)

![PR XGBoost](images/pr_xgboost.png)
ğŸ–¼ï¸ Image Assets Summary (Optional but Professional)

## ğŸ–¼ï¸ Visualization Assets

| Image | Description |
|------|-------------|
| `before_smote.png` | Confusion matrix before SMOTE |
| `after_smote.png` | Confusion matrix after SMOTE |
| `roc_curve.png` | ROC curve |
| `pr_curve.png` | Precisionâ€“Recall curve |
| `bussiness_threesold.png` | Business threshold confusion matrix |
| `xg_boost.png` | XGBoost model performance |
| `pr_xgboost.png` | Precisionâ€“Recall curve for XGBoost |


ğŸ§° Technologies & Libraries Used

Python

NumPy

Pandas

Scikit-learn

Imbalanced-learn (SMOTE)

XGBoost

Matplotlib

Seaborn

ğŸš€ Key Learnings

Accuracy is unreliable for imbalanced datasets

Handling class imbalance is essential in fraud detection

SMOTE significantly improves minority class detection

Threshold tuning is more important than raw scores

Advanced models like XGBoost capture complex fraud patterns

Real-world ML requires balancing performance and constraints

ğŸ”® Future Improvements

Ensemble learning (Logistic + XGBoost)

Hyperparameter tuning

Time-aware validation

SHAP explainability

Real-time fraud detection API

Web deployment using Streamlit

âœ… Conclusion

This project demonstrates a complete, real-world fraud detection pipeline using binary classification.
It emphasizes correct evaluation, imbalance handling, and engineering decisions, making it suitable for:

Academic submission

GitHub portfolios

Technical interviews
